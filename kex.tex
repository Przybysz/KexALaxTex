
\documentclass{article}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage[utf8]{inputenc}

\begin{document}

\section{Introduction}
Forecasting the behavior of financial products is an important preoccupation for many professionals and laymen alike, as well as academics. Investment banks, hedge funds, and insurance companies spend significant time and effort in predicting changes in the properties, such as prices and risks, of stocks, bonds, options, etc. This interest in financial markets has for the last few decades driven the development of better tools for analyzing financial information. Most fruitful, historically, has perhaps the research in financial mathematics been. 

Technical analysis is the study of past information to predict future values. Technical analysis is commonly used on financial markets and can be used on for example stock prices. Mathematical concepts are used, as an example ARMIA [KÄLLA], however the mathematical sophistication of these are generally low. Technical analysis also takes the form of grouping continuous data into discrete packets of data to visually summarize the data, sometimes referred to as candlesticks [KÄLLA].

Candlestick charts are frequently used in technical analysis to represent time series. Time series are divided into specific units, most commonly days, and transformed into “candlesticks”. Each candlestick holds information about the starting and final value of that time period as well as the highest and lowest levels reached in between. A candlestick can thereby be summarized as start and end values combined with the volatility of the underlying value.

Financial mathematics is the use of advanced mathematical and statistical methods to price and minimize risk of financial products and portfolios. An example of this is Black-Scholes method for options pricing [KALLA] and the LIBOR market model for pricing interest rates derivatives [KALLA]. With the increasing access to computational power the use of artificial intelligence, particularly machine learning has become an important aspect of market prediction. 

Machine learning has been successfully used for time-series financial forecasting. Support vector machines (SVM) and artifical neural networks (ANN) have been widely used  within academics to predict financial data. Cao and Tays work on SVMs and ANNs cemented SVMs as an important tool for stock prediction. The SVM has since been expanded upon to form a multiple-kernel based learning system for stock prediction (Fletcher, 2012) (Yeh, et al., 2010) and credit risk evaluation (Yu, et a.l, 2009), and the extreme learning machines (ELM), a type of ANN, has been developed and used for financial prediction (Ding, et al 2015). Simpler approches such as desicion trees have also been successfully used (Audrino, et al 2010), whereas there's very little research done on the Naive-Bayes classifier for stock data.

There has been little research on the performance of various combinations of machine learning and technical analysis methods. The research that has been made on the subject is generally focused on one, or a few, lerning algorithms and little weight is put on the the importance of the choice of parameters. As such there is a gap of knowledge as to which parameters work well with which algorithms. It is our ambition to contribute to filling that gap by analysing the performance of a few selected algorithms from different machine learning paradigms with alternating input parameters.

\subsection{Problem statement}
What is the performance of the SVM, ELM, Random-Forest, and Naive-bayes algorithms on financial data with respect to changes in price, volatility, trading volume, and different time intervals, as well as combinations of these?


\section{Background}
\subsection{Technical analysis}
Technical analysis is a method for anticipating market price changes by studying historic data for that market. It primarily focuses on past price levels, but trading volumes are used as well. The fundamental assumption of technical analysis is that market prices reflect all available information [KÄLLA UNDER TEXTEN] separating it from other security analysis methodologies such as fundamental analysis, where the state of the underlying asset is ascertained by reviewing its financial statements. This reflection is not seen as perfect because the relevant information might not always be wholly available due to, for example, insider trading [SAMMA]. 
\\
According to technical analysis, prices move in distinct trends. A trend is bullish if the highest and lowest price levels for a security are gradually higher and bearish if the opposite is true. A flat trend occurs when price levels stay the same during a longer time interval.
\\
A third concept of technical analysis is that history repeats itself and "the future is just a repetition of the past" [inre källa från källan] and so past errors and events, such as market crashes or speculative bubbles, might be repeated by new generations.
\\
There is no consensus whether technical analysis can be effectively used as a investment strategy. Studies have been made that show technical analysis to be no better than simply buying a security and holding it until an opportune time, without any portfolio strategy.

http://onlinelibrary.wiley.com.focus.lib.kth.se/doi/10.1002/9781118467220.ch8/pdf
\subsubsection{Volatility}
Volatility has been widely used in financial market prediction. Many models within mathematical finance, including the Black-Scholes model, use volatility as one of the main components for handeling pricing. Machine learning algorithms have been used to predict future volatility, including SVMs (Wang, et al. 2011), ELMs (Wang, et al. 2014), and regression trees (Audrino, et al. 2009).  However little research on the predictive capabilities of using volatility as an indicator for future market behavior has been done. In this paper we will examine how using volatility as a predictor, on several different machine learning algorithms, applied on stock price data influences the accuracy in prediction of future price developement. 

\subsubsection{Volume}
The volume of trades, i.e. the number of trades done during the time interval of some underlying asset, is one of the main aspects of technical analysis. The general idea is that changes in price of the asset together with a small volume will have a different impact on the market than the same change in price associated with a larger trading volume. 

\subsection{SVM}
The SVM algorithm is effectively trying to place a high-dimensional plane, or hyperplane, between the differently classified sets of datapoints and predicting new datapoints based on which side of the hyperplane they are located. Data is however often not dividable by a plane, which the SVM handles using slack variables and kernels. \\
The aim of a basic seperating hyperplane algorithm is to seperate the different classes of data using a hyperplane, while achieving the maximum margin between the closest data points of the different classes and the seperating plane. SVM is an extension of the seperating hyperplane idea by representing the data in higher dimensions to spread the distrinution of the low dimensional data. This can be summarized as the following optimization problem;
\begin{equation}
\min \frac{1}{2}\vec{w}^{T}\vec{w}
\end{equation} 
\begin{center}
 subject to  $t_i \vec{w}^{T} \Phi(\vec{x}_i)  \geq 1$,  \\
\end{center}
where $\vec{w}$ is the weight vector of the hyperplane, $\vec{x}_i$ is the $ith$ training point, $t_i$ is the class of the $ith$ training point, and $\Phi(\vec{x})$ is the non-linear transformation used. This formulation is however often too rigid, and unable to find an optimal solution if there are data points from different classes intertwined. To deal with this problem slack variables are introduced and the problem is rewritten as; 
\begin{equation}
\min \frac{1}{2}\vec{w}^{T}\vec{w} + C\sum\limits_i \zeta_i 
\end{equation}
\begin{center}
subject to $t_i \vec{w}^{T} \Phi(\vec{x}_i)  \geq 1 - \zeta_i, $
\end{center}
where $\zeta_i$ are the slack variables and $C$ is a slack constant. Also this formulation can be restated, in the form of it's dual problem;

\begin{equation}
\max \sum\limits_i \alpha_i - \frac{1}{2}\sum\limits_{i,j} \alpha_i \alpha_j t_i t_j \kappa (\vec{x_i}, \vec{x_j}) 
\end{equation}
\begin{center}
subject to $0 \leq \alpha_i \leq C, $
\end{center}
where $\kappa (\vec{x_i}, \vec{x_j}) = \Phi(\vec{x_i})^T \Phi(\vec{x_j})$ is the kernel. Once the optimization problem has been solved, i.e. the model has been trained, unseen data points, $\vec{x}$,  are classified with $\sum\limits_i \alpha_i t_i \kappa(\vec{x},\vec{x_i}) > 0$. [K\"ALLA P\AA\ ALLT DET H\"AR, TOG DET FR\AA N ML-KURSENS SLIDES]


\subsection{Random Forest}
Random forest is a classification algorithm belonging to the tree-based machine learning algorithms. Tree-based methods attempt to divide the space of possible outcomes into generally simple areas. This is done by splitting the outcome space in two, one feature at a time. Tree-based methods are believed to reflect the way humans make decisions. They also allow for easy inference even in its simplest forms, but usually need to be improved in order to reach prediction accuracy rates comparable to other algorithms. Such improvement can be attained by implementing Random Forest.

[KÄLLA ML-BOKEN]
\subsubsection{Decision trees}
KANSKE MÅSTE FÖRENKLA LITE HÄR
The decision tree classifier (also a regressor) constructs a logical sqeuent, in the form of a tree data structure. The idea is to identify the attribute, at the current node in the tree, on which one can gain the most information from splitting on. This is done by comparing the entropy of the data set before and after the split, i.e. the information gain of the split;
\begin{equation}
I(S, A) = Ent(S) - \sum\limits_{v \in Values(A)} \frac{|S_v|}{|S|} Ent(S_v),
\end{equation}
where $S$ is the training set, $A$ is a attribute of the training set, and $S_v$ is the set after the split containing the value $v$ on attribute $A$. $Ent(S)$ is the entropy of the set S. Once the tree has been trained the sequent formed is used to classify unseen data points.
\subsubsection{Bagging}
Decision trees can attain very different results even if they are trained on the same dataset making them unpredictable. [BOK] Bootstrap aggregation, bagging in short, is an attempt to make decision trees more predictable when trained on the same dataset in order to achieve consistent prediction accuracy rates. \\ \\
Bagging uses bootstrapped replicates of the training set to train several instances of and combine a simple classifier, $f_b$ to form a more powerful predictor, $f_{bag}$. This is done by creating $B$ bootstrapped training sets, $S_b$, by sampling with replacement from the given training set $S = \{ (\vec{x}_1, y_1), ..., (\vec{x}_m, y_m)\}$, where possibly $|S_b| > m$. The classifier $f_b$ is then trained using $S_b$, and unseen data points are classified using; 
\begin{equation}
f_{bag}(\vec{x}) = \max\limits_k \sum\limits_b Ind(f_b(\vec{x}) = k),
\end{equation}
where $k$ is the class, and $Ind(a = b) = 1$ if $a = b$ and $Ind(a = b) = 0$ if $a \ne b$.

\subsubsection{Random forest}
The random forest method is similar to bagged trees, it uses bootstrapping to construct several training sets and subsequently trains decision trees on these to create a classifier with greater accuracy than the individual decision trees. It differs from bagging in the way the predictors are used in splitting. Whenever a split in a tree is considered, only a random subset $n$ of the available predictors are provided as possible candidates for splitting. This is done to prevent the algorithm from creating very similar trees due to one particularly strong predictor always being chosen in the splitting process.
 
[BOK] [K\"ALLA P\AA\ ALLT DET H\"AR, TOG DET FR\AA N ML-KURSENS SLIDES]

\subsection{Extreme learning machines}
The extreme learning machine is a type of ANN, namely a single layer feedforward network (SLFN). This means that the algorithm takes input and pushes it forward to hidden nodes, who in turn outputs a classification for new datapoints. The standard mathematical representation of an SLFN is;
\begin{equation}
\sum\limits_{i = 1}^N \vec{\beta}_i f_i (\vec{x}_j)  = \sum\limits_{i = 1}^N \vec{\beta}_i f_i (\vec{a}_i  \vec{x}_j + b_i)= t_j, j = 1...n, 
\end{equation}
where $N$ is the number of nodes in the hidden layer, $\vec{a}_i$ is the is the input-weight vector for the $i$th node, $\vec{\beta}_i$ is the output-weight vector for the $i$th node, $b_i$ is the threshold of the $i$th node, and $f_i(\vec{x})$ is the activation function (or kernel in SVM terminology). This can in turn be written comapctly as $H\beta = T$, where $H$ is an $n \times N$ matrix and $H_{i, j} = f(\vec{a}_j\vec{x}_i + b_j)$, $\beta$ is a vector with $\beta_i^T$ in position $i$, and $T$ is a vector with the sample outputs. To train the classifier one initiates all $\vec{a}_i$ and $b_i$ to random values, and compute the output-weight vector $\hat{\beta}$ from $\hat\beta = H^+ T$, where $H^+$ is the Moore-Penrose pseudo-inverse of $H$ (Li, et al 2016).

\subsection{Naive Bayes}
The naive bayes classifier is a probability based classifier based on Bayes' Theorem. The main idea is to, instead of fitting $k$-dimensional data in to one probability distribution, to fit the data into $k$ separate, one-dimensional  distributions. In order to use the algorithm, a critical assumption has to be made, namely that the features are regarded as independent. This assumption is what makes this algorothm 'naive'. Using the maximum a priori method (MAP), the naive bayes can be implemented as follows:
\begin{center}
$Y_{MAP} = \arg \max_{y \in Y} P(y | x_1, ..., x_k) = arg \max_{y \in Y} \frac{ P( x_1, ..., x_k | y)P(y) }{P(x_1, ..., x_k)}=$
\end{center}
\begin{equation}
 = arg \max_{y \in Y} P(x_1, ..., x_k|y)P(y) ,
\end{equation}
where $\vec{x} = (x_1, ..., x_k)$ is a $k$-dimensional data point, and $Y = \{1, 2, ...,y_n\}$ are the possible classes. Since the features are regarded as independent we are assuming that $P(x_1, ..., x_k| y) = \prod_{k=1}^{K} P(x_k |y).$ As such the classifier becomes $Y_{MAP} = arg \max_{y \in Y} P(y)\prod_{k=1}^{K}P(x_k|y)$.

\subsection{Cross-validation}
Cross-validation is used to ensure that the prediction accuracy of a learning algorithm is properly estimated. The data set is split into $p$ partitions and the algorithm is trained on$p-1$ of the subsets and then the predictive accuracy of the algorithm is tested on the remaining subset. This process is repeated $p$ times, called $p$-fold cross-validation, using every subset as the testing set once. The performance of the different tests are then averaged.

\newpage

\section{Method}
The aim of this paper is to evaluate the influence of using a volatility and volume measure as an indicator of future stock price developements. This will be done by running the different algorithms on sample data sets using firstly only past end-of-day stock prices for different number of days looking in the past. Then it will be also be run on price and volatility measure combined, price and volume combined, and lastly price, volatility and volume measures combined. 

\subsection{Price}
The price used will be normalized, i.e. every end-of-day stock price will in fact be a percentage of the start-of-day price.

\subsection{Volatility measure}
Due to the lack of precision data the volatility will have to be approximated. In this paper we will approximate the volatility by letting the highest and lowest prices of the trading day be parameters of the data used to train the machine learning algorithms. As with the price we also here use the percentage of the start-of-day price.

\subsection{Volume}
The volume parameter is also normalized. This is done by taking the average trading volume over the entire time interval and dividing each individual volume data point by this number. 

\subsection{Days looking backwards}
The algorithms are run on 5 data input using five different time aspects. These are looking backwards on the 2, 3, 5, 10, and 30 previous trading days. The various combinations mentioned above are implemented on each of theprevious days. This gives, as an example, nine features per data point for the algorithms running price and volatility (highest and lowest points) looking backwards three days.

\subsection{Data set}
The algoithms will be run on the S\&P500 index. The data set contains daily trading information for the stocks from January 3 2000 to 27 March 2016, including opening and closing prices, highest and lowest prices, and dividend adjusted price change. 

\subsection{Accuracy} 
To evaluate the performance of the algorithms we look at the prediction accuracy. Two different approaches to determining the significance ofthe accuracy are used. The ELM algorithm is run ten times on the different data sets and an average of the prediction accuracy is taken to ensure a representative value. On the other three algorithms ten-fold cross-validation is used. 
\newpage

\section{Experimental results}
\subsection{SVM}
The following tables show the accuracy of the SVM using the different combinations of input features. The accuracy is given in percentages.
\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 50.9679  \\ \hline
    3 & 51.0539  \\ \hline
    5 & 51.4468  \\ \hline
    10 & 51.3135  \\ \hline
    30 & 51.6408 \\ 
    \hline
    \end{tabular}
\caption{SVM prediction accuracy on price features}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 51.2129  \\ \hline
    3 & 51.6176  \\ \hline
    5 & 51.2997  \\ \hline
    10 & 51.3135  \\ \hline
    30 & 51.3694 \\ 
    \hline
    \end{tabular}
\caption{SVM prediction accuracy on price and volatility features}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 51.1149  \\ \hline
    3 & 51.4706  \\ \hline
    5 & 51.2016  \\ \hline
    10 & 50.9452  \\ \hline
    30 & 51.3447 \\ 
    \hline
    \end{tabular}
\caption{SVM prediction accuracy on price and volume features}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 51.2374  \\ \hline
    3 & 51.5441  \\ \hline
    5 & 51.3242  \\ \hline
    10 & 50.9944  \\ \hline
    30 & 51.3694 \\ 
    \hline
    \end{tabular}
\caption{SVM prediction accuracy on price, volatility and volume features}
\end{center}
\end{table}

\includegraphics[width=1.25\textwidth, center]{svm_graph.png}

As seen, all configurations aside from price only lead to similar prediction accuracy rates. Accuracy rates range approximately between $51\%$ and  $51.7\%$. The highest score was attained by the 30-day price only configuration, followed by the 3-day price and volatility set. The price only configuration is the only one that appears to have more accurate results with the increase of lookback [ÄNDRA KANSKE] days.
\newpage
\subsection{ELM}
The following tables show the accuracy of the ELM using the different combinations of input features. The accuracy is given in percentages.
\\
\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 &  51.7 \\ \hline
    3 & 53.05  \\ \hline
    5 & 53.26  \\ \hline
    10 & 51.91  \\ \hline
    30 & 53.1 \\ 
    \hline
    \end{tabular}
\caption{ELM prediction accuracy on price features}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 54.15  \\ \hline
    3 & 51.9  \\ \hline
    5 & 51.63  \\ \hline
    10 & 51.33  \\ \hline
    30 & 52.61 \\ 
    \hline
    \end{tabular}
\caption{ELM prediction accuracy on price and volatility features}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 52.37  \\ \hline
    3 & 51.63  \\ \hline
    5 & 50.66  \\ \hline
    10 & 51.33  \\ \hline
    30 & 52.3 \\ 
    \hline
    \end{tabular}
\caption{ELM prediction accuracy on price and volume features}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 53.54  \\ \hline
    3 & 53.61  \\ \hline
    5 & 52.24  \\ \hline
    10 & 52.91  \\ \hline
    30 & 50.96 \\ 
    \hline
    \end{tabular}
\caption{ELM prediction accuracy on price, volatility and volume features}
\end{center}
\end{table}
\includegraphics[width=1.25\textwidth, center]{elm_graph.png}

Here, no one configuration of features consistently outperforms any of the others. Accuracy rates range approximately between $51\%$ and $54\%$. The highest rate is achieved by the 2-day price and volatility set but has lower rates than both the price only and the complete configuration when the number of lookback days [ÄNDRA KANSKE] increases.

\newpage
\subsection{Random Forest}
The following tables show the accuracy of the Random-Forest algorithm using the different combinations of input features. The accuracy is given in percentages.
\\
\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 51.37  \\ \hline
    3 & 50.01  \\ \hline
    5 & 50.49  \\ \hline
    10 & 50.87  \\ \hline
    30 & 52.15 \\ 
    \hline
    \end{tabular}
\caption{Random-Forest prediction accuracy on price features}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 52.07  \\ \hline
    3 & 50.72  \\ \hline
    5 & 51.21  \\ \hline
    10 & 52.17  \\ \hline
    30 & 52.02 \\ 
    \hline
    \end{tabular}
\caption{Random-Forest prediction accuracy on price and volatility features}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 51.47  \\ \hline
    3 & 52.75  \\ \hline
    5 & 51.78  \\ \hline
    10 & 51.38  \\ \hline
    30 & 51.7 \\ 
    \hline
    \end{tabular}
\caption{Random-Forest prediction accuracy on price and volume features}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 51.81  \\ \hline
    3 & 51.68  \\ \hline
    5 & 51.34  \\ \hline
    10 & 52.14  \\ \hline
    30 & 52.32 \\ 
    \hline
    \end{tabular}
\caption{Random-Forest prediction accuracy on price, volatility and volume features}
\end{center}
\end{table}
\includegraphics[width=1.25\textwidth, center]{random_forest_graph.png}

Accuracy rates range approximately between $50\%$ and $53\%$. All configurations except price and volume follow a similar pattern. The 3-day price and volume achieves the best accuracy, followed by the 30-day price volatility and volume configuration.

\newpage
\subsection{Naive-Bayes}
The following tables show the accuracy of the Naive-Bayes algorithm using the different combinations of input features. The accuracy is given in percentages.
\\
\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 53.00  \\ \hline
    3 & 52.73  \\ \hline
    5 & 52.78  \\ \hline
    10 & 53.13  \\ \hline
    30 & 52.42 \\ 
    \hline
    \end{tabular}
\caption{Naive-Bayes prediction accuracy on price features}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 52.50  \\ \hline
    3 & 52.56  \\ \hline
    5 & 52.35  \\ \hline
    10 & 52.12  \\ \hline
    30 & 52.14 \\ 
    \hline
    \end{tabular}
\caption{Naive-Bayes prediction accuracy on price and volatility features}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 52.66  \\ \hline
    3 & 52.30  \\ \hline
    5 & 51.80  \\ \hline
    10 & 51.74  \\ \hline
    30 & 52.13 \\ 
    \hline
    \end{tabular}
\caption{Naive-Bayes prediction accuracy on price and volume features}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
    \begin{tabular}{ | l | l | p{3cm} |}
    \hline
    Days looking backward & Accuracy \\ \hline
    2 & 53.01  \\ \hline
    3 & 52.42  \\ \hline
    5 & 51.93  \\ \hline
    10 & 52.29  \\ \hline
    30 & 52.69 \\ 
    \hline
    \end{tabular}
\caption{Naive-Bayes prediction accuracy on price, volatility and volume features}
\end{center}
\end{table}

\includegraphics[width=1.25\textwidth, center]{bayes_graph.png}

Accuracy rates range approximately between $51.5\%$ and $53\%$. The 10-day price only configuration achieves the best accuracy, followed by a tie between the 2-day price only configuration and the 2-day complete configuration.

\newpage

\section{Discussion}
\subsection{SVM}
There are trends in the data. All features combinations perform better for 3 days looking backwards than 2 days. The price only measure has a general upwards curve of prediction accuracy with more days looking backwards. This indicates that 2 days is suboptimal for predcition. The price only measure is outperformed by all other measures on 2 and 3 days, but outperformes all other measures for 5 days and more, with the exception of price and volatility for 10 days. This indicates that the additional information is useful on a smaller timescale, but impairs prediction accuracy on longer ones by creating unnecessary noise. The exception, as mentioned, being price and volatility on 10 days. This, together with the fact that price and volatility constantly outperforms the volume included measures, suggests that the volatility measure is more reliable than the volume measure. However, since the top performing measure is the price only for 30 days implies that volatility only is reliable for shorter time frames.

\subsection{ELM}
The performance of the price and volatility and price and volume measures have similar developments over difference in days, with the price and volatility measure constantly performing better or on par with price and volume. This indicates that volatility is a better faeture for indicating stock behavior. However, the price only measure in turn outperforms both of these for 3 and more days. The price, volatility and volume measure performs at or around the best measure for all days except 30. This decline in accuracy could be caused by the large number of input parameters looking back 30 days. Should this be the case, it would seem the ELM algorithm is able to use both volatility and volume together to create a measure better than the individual measures. 

\subsection{Random Forest}
All measures perform comparably for 2 and 30 days. The price only measure is outperformed on all timescales in between, which implies that the addition of information is beneficial for the predictive accuracy of the random forest algorithm. The addition of volume gives rise to siginificant rise in accuracy for 3 days and also outperforms all other measures for 5 days. The volatility, however,  measure performs better for 10 and 30 days. The combined volatility and volume measure performs better or on par with the volatility measure for all timeframes. This signals that the volume measure should be used for all number of days, with the addition of the volatility measure when using more than 5 days of looking backwards, to create the optimal random forest stock predictor.

\subsection{Naive Bayes}
The price only measure performs better, or marginally worse, than all other combinations of measure for all days, with a sharp decline in accuracy from 10 day to 30 days. This suggests that the addition of features impairs the performance of the naive bayes classifier, for stock data. There is significant differnce in performance of the the volatility and volume measures, compared to the price only measure. However the combined price, volatility and volume measure outperforms the price only measure for 30 days. This could indicate that more data features increase accuracy for longer timescales. All meaures perform worse on the 30 day timeframe compared to the 2 day timeframe, signalling that even though more data features increase accuracy with more days, it might not be worth it. 

\section{Conclusions}
H\"ar drar vi en slutsats baserat p\aa\ diskussionen kring resultaten av vad vi gjort.
\end{document}